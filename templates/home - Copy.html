<!-- <!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sign Language Detection</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            text-align: center;
            padding: 20px;
        }
        .container {
            max-width: 600px;
            margin: auto;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1);
        }
        .start-btn {
            background-color: #007bff;
            color: white;
            padding: 10px 20px;
            border: none;
            border-radius: 5px;
            text-decoration: none;
            font-size: 18px;
            margin-top: 20px;
            display: inline-block;
        }
        .start-btn:hover {
            background-color: #0056b3;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Welcome to Sign Language Detection</h1>
        <p>This project aims to detect and recognize hand signs in real-time using computer vision and machine learning.</p>
        <p>Click the button below to start the live demo.</p>
        <a href="{{ url_for('index') }}" class="start-btn">Start Demo</a>
    </div>
</body>
</html> -->


<!-- <!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sign Language Detection</title>
    <style>
        /* Global Styles */
        body {
            font-family: 'Arial', sans-serif;
            background-color: #f4f4f4;
            margin: 0;
            padding: 0;
            text-align: center;
        }
        header {
            background: #007bff;
            color: white;
            padding: 15px;
            font-size: 22px;
            font-weight: bold;
        }
        .container {
            max-width: 900px;
            margin: 30px auto;
            padding: 20px;
            background: white;
            border-radius: 12px;
            box-shadow: 0 0 15px rgba(0, 0, 0, 0.2);
            animation: fadeIn 1s ease-in-out;
        }
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(-20px); }
            to { opacity: 1; transform: translateY(0); }
        }
        h1 {
            color: #333;
            font-size: 30px;
        }
        h2 {
            color: #007bff;
            font-size: 24px;
            margin-top: 20px;
        }
        p {
            font-size: 18px;
            color: #555;
            line-height: 1.6;
        }
        .step {
            background: #e3f2fd;
            padding: 15px;
            margin: 15px 0;
            border-radius: 10px;
            text-align: left;
        }
        .step-title {
            font-size: 22px;
            font-weight: bold;
            color: #007bff;
        }
        .step p {
            font-size: 17px;
            color: #444;
        }
        .btn {
            display: inline-block;
            padding: 12px 20px;
            font-size: 18px;
            text-decoration: none;
            border-radius: 8px;
            transition: 0.3s;
            cursor: pointer;
            margin-top: 10px;
        }
        .start-btn {
            background-color: #007bff;
            color: white;
        }
        .start-btn:hover {
            background-color: #0056b3;
        }
        .train-btn {
            background-color: #28a745;
            color: white;
        }
        .train-btn:hover {
            background-color: #218838;
        }
    </style>
</head>
<body>

<header>
    Sign Language Detection Project
</header>

<div class="container">
    <h1>Welcome to Sign Language Detection</h1>
    <p>This project uses computer vision and machine learning to detect hand signs in real-time.</p>

    <h2>Project Steps</h2>

    <div class="step">
        <span class="step-title">Step 1: Data Collection</span>
        <p>We use OpenCV to capture images of different hand gestures. Each sign needs multiple images for better accuracy.</p>
        <p>ðŸ‘‰ Press <b>'S'</b> while running the data collection script to save an image.</p>
    </div>

    <div class="step">
        <span class="step-title">Step 2: Preprocessing</span>
        <p>The collected images are cropped and resized to a fixed size (300x300 pixels). This ensures uniform input for the model.</p>
        <p>Images are converted to grayscale or normalized for better training results.</p>
    </div>

    <div class="step">
        <span class="step-title">Step 3: Model Training</span>
        <p>We use a machine learning model (TensorFlow/Keras) to classify hand signs. The model learns patterns from the processed images.</p>
        <p>Train the model using Google's <b>Teachable Machine</b> for a user-friendly training experience.</p>
        <a href="https://teachablemachine.withgoogle.com/train/image" class="btn train-btn" target="_blank">Train Your Model</a>
    </div>

    <div class="step">
        <span class="step-title">Step 4: Real-Time Detection</span>
        <p>Using OpenCV and the trained model, hand signs are detected in real-time and classified instantly.</p>
        <p>The recognized sign is displayed along with its meaning and an image representation.</p>
    </div>

    <a href="{{ url_for('index') }}" class="btn start-btn">Start Demo</a>
</div>

<footer style="margin-top: 20px; padding: 10px; background: #007bff; color: white; text-align: center;">
    &copy; 2025 Sign Language Detection | Developed by Vishal Tawase
</footer>

</body>
</html> -->


<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sign Language Detection</title>
    <style>
        /* Global Styles */
        body {
            font-family: 'Arial', sans-serif;
            background-color: #f4f4f4;
            margin: 0;
            padding: 0;
            text-align: center;
        }
        header {
            background: #007bff;
            color: white;
            padding: 15px;
            font-size: 22px;
            font-weight: bold;
        }
        .container {
            max-width: 1300px;
            margin: 30px auto;
            padding: 20px;
            background: white;
            border-radius: 12px;
            box-shadow: 0 0 15px rgba(0, 0, 0, 0.2);
            animation: fadeIn 1s ease-in-out;
        }
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(-20px); }
            to { opacity: 1; transform: translateY(0); }
        }
        h1 {
            color: #333;
            font-size: 30px;
        }
        h2 {
            color: #007bff;
            font-size: 24px;
            margin-top: 20px;
        }
        p {
            font-size: 18px;
            color: #555;
            line-height: 1.6;
        }
        .step {
            background: #e3f2fd;
            padding: 15px;
            margin: 15px 0;
            border-radius: 10px;
            text-align: left;
        }
        .step-title {
            font-size: 22px;
            font-weight: bold;
            color: #007bff;
        }
        .step p {
            font-size: 17px;
            color: #444;
        }
        .btn {
            display: inline-block;
            padding: 12px 20px;
            font-size: 18px;
            text-decoration: none;
            border-radius: 8px;
            transition: 0.3s;
            cursor: pointer;
            margin-top: 10px;
        }
        .start-btn {
            background-color: #007bff;
            color: white;
        }
        .start-btn:hover {
            background-color: #0056b3;
        }
        .train-btn {
            background-color: #28a745;
            color: white;
        }
        .train-btn:hover {
            background-color: #218838;
        }
        .code-box {
            background: #222;
            color: #f8f8f2;
            font-family: 'Courier New', monospace;
            padding: 15px;
            border-radius: 8px;
            text-align: left;
            overflow-x: auto;
        }
        .code-box pre {
            margin: 0;
            font-size: 16px;
            white-space: pre-wrap;
        }
    </style>
</head>
<body>

<header>
    Sign Language Detection Project
</header>

<div class="container">
    <h1>Welcome to Sign Language Detection</h1>
    <p>This project uses computer vision and machine learning to detect hand signs in real-time.</p>

    <h2>Project Steps</h2>

    <!-- <div class="step">
        <span class="step-title">Step 1: Data Collection</span>
        <p>We use OpenCV to capture images of different hand gestures. Each sign needs multiple images for better accuracy.</p>
        <p>ðŸ‘‰ Press <b>'S'</b> while running the data collection script to save an image.</p>

        <h3>Project Folder Structure:</h3>
        <div class="code-box">
            <pre>
Signlanguagedetection/
â”‚â”€â”€ Data/
â”‚   â”œâ”€â”€ Hello/
â”‚   â”œâ”€â”€ No/
â”‚   â”œâ”€â”€ Okay/
â”‚   â”œâ”€â”€ Please/
â”‚   â”œâ”€â”€ Thank you/
â”‚   â”œâ”€â”€ Yes/
â”‚â”€â”€ Model/
â”‚   â”œâ”€â”€ keras_model.h5
â”‚   â”œâ”€â”€ labels.txt
â”‚â”€â”€ dataCollection.py
â”‚â”€â”€ test.py
â”‚â”€â”€ pyvenv.cfg
            </pre>
        </div>
    </div> -->

    <!-- <div class="step">
        <span class="step-title">Step 1: Data Collection - Capturing Hand Gestures for Sign Language Detection</span>
        <p>
            This step focuses on gathering a comprehensive dataset of hand gestures, which is crucial for training our sign language detection model. We utilize OpenCV, a powerful library for computer vision, to capture real-time video from your webcam. The <code>cvzone.HandTrackingModule</code> is employed to detect and track hand movements within the video frames.
        </p>
    
        <h3>Detailed Explanation of the <code>dataCollection.py</code> Script:</h3>
        <p>
            The Python script <code>dataCollection.py</code> performs the following key operations:
        </p>
        <ol>
            <li>
                <strong>Webcam Initialization:</strong>
                <p>
                    The script starts by initializing the webcam using <code>cv2.VideoCapture(0)</code>. This opens the default camera (index 0) and prepares to capture video frames.
                </p>
            </li>
            <li>
                <strong>Hand Detection Setup:</strong>
                <p>
                    A <code>HandDetector</code> object from the <code>cvzone</code> library is created with <code>maxHands=1</code>, indicating that we are tracking only one hand at a time. This detector will locate and provide bounding box information for the hand.
                </p>
            </li>
            <li>
                <strong>Image Processing Parameters:</strong>
                <p>
                    Several parameters are defined:
                    <ul>
                        <li><code>offset = 20</code>: This adds a margin around the hand's bounding box to ensure the entire hand is captured.</li>
                        <li><code>imgSize = 300</code>: This sets the desired size (in pixels) for the processed hand images, ensuring uniformity in the dataset.</li>
                        <li><code>counter = 0</code>: This variable tracks the number of images captured for each gesture.</li>
                        <li><code>folder = "C:\\Users\\VISHAL TAWASE\\Downloads\\VIT ALL SEM\\Winter Sem 2023-24\\ML for Robotics\\Project\\env\\env2\\Sign language detection\\Data\\Hello"</code>: This specifies the directory where the captured images will be saved. Make sure to replace this with the correct path corresponding to the gesture being captured. For example, if you are capturing the "Hello" gesture, you should point to the "Hello" folder inside the Data directory.</li>
                    </ul>
                </p>
            </li>
            <li>
                <strong>Main Loop and Hand Detection:</strong>
                <p>
                    The <code>while True</code> loop continuously captures frames from the webcam. The <code>detector.findHands(img)</code> function detects hands in each frame, returning the hand data and the modified image.
                </p>
            </li>
            <li>
                <strong>Hand Bounding Box and Image Cropping:</strong>
                <p>
                    If a hand is detected, the script extracts the bounding box coordinates (<code>x</code>, <code>y</code>, <code>w</code>, <code>h</code>) and crops the hand region from the original image with the added offset.
                </p>
            </li>
            <li>
                <strong>Image Resizing and Padding:</strong>
                <p>
                    To ensure all hand images have the same size, the script resizes and pads the cropped image. It calculates the aspect ratio of the hand and resizes the image accordingly. Then, it creates a white canvas (<code>imgWhite</code>) of the desired size and places the resized hand image in the center, padding the remaining areas with white pixels. This ensures consistent input for the model.
                </p>
            </li>
            <li>
                <strong>Displaying Images:</strong>
                <p>
                    The script displays the cropped hand image (<code>ImageCrop</code>) and the padded, resized image (<code>ImageWhite</code>) using <code>cv2.imshow()</code>. It also shows the original webcam feed (<code>Image</code>).
                </p>
            </li>
            <li>
                <strong>Saving Images:</strong>
                <p>
                    When the user presses the 'S' key, the <code>imgWhite</code> image is saved to the specified folder with a unique filename (<code>Image_{time.time()}.jpg</code>). The <code>counter</code> is incremented, and the current count is printed to the console.
                </p>
            </li>
            <li>
                <strong>Loop Continuation:</strong>
                <p>
                    The <code>cv2.waitKey(1)</code> function waits for a key press for 1 millisecond. If no key is pressed, the loop continues to the next frame.
                </p>
            </li>
        </ol>
    
        <p>
            <strong>Important Note:</strong>
            Before running the script, ensure you have the necessary libraries installed (OpenCV and cvzone). Also, adjust the <code>folder</code> variable to match the correct directory for each gesture you want to capture.
        </p>
    
        <p>
            ðŸ‘‰ Press <b>'S'</b> while running the <code>dataCollection.py</code> script to save an image of the current hand gesture. Capture multiple images for each gesture to create a robust dataset.
        </p>
    
        <h3>Project Folder Structure:</h3>
        <div class="code-box">
            <pre>
    Signlanguagedetection/
    â”‚â”€â”€ Data/
    â”‚   â”œâ”€â”€ Hello/
    â”‚   â”œâ”€â”€ No/
    â”‚   â”œâ”€â”€ Okay/
    â”‚   â”œâ”€â”€ Please/
    â”‚   â”œâ”€â”€ Thank you/
    â”‚   â”œâ”€â”€ Yes/
    â”‚â”€â”€ Model/
    â”‚   â”œâ”€â”€ keras_model.h5
    â”‚   â”œâ”€â”€ labels.txt
    â”‚â”€â”€ dataCollection.py
    â”‚â”€â”€ test.py
    â”‚â”€â”€ pyvenv.cfg
            </pre>
        </div>
    </div> -->

    <div class="step">
        <span class="step-title">Step 1: Data Collection - Capturing Hand Gestures for Sign Language Detection</span>
        <p>
            This step focuses on gathering a comprehensive dataset of hand gestures, which is crucial for training our sign language detection model. We utilize OpenCV, a powerful library for computer vision, to capture real-time video from your webcam. The <code>cvzone.HandTrackingModule</code> is employed to detect and track hand movements within the video frames.
        </p>
    
        <button id="detailsButton">Show Detailed Explanation of dataCollection.py</button>
    
        <div id="detailsContent" style="display: none;">
            <h3>Detailed Explanation of the <code>dataCollection.py</code> Script:</h3>
            <p>
                The Python script <code>dataCollection.py</code> performs the following key operations:
            </p>
            <ol>
                <li>
                    <strong>Webcam Initialization:</strong>
                    <p>
                        The script starts by initializing the webcam using <code>cv2.VideoCapture(0)</code>. This opens the default camera (index 0) and prepares to capture video frames.
                    </p>
                </li>
                <li>
                    <strong>Hand Detection Setup:</strong>
                    <p>
                        A <code>HandDetector</code> object from the <code>cvzone</code> library is created with <code>maxHands=1</code>, indicating that we are tracking only one hand at a time. This detector will locate and provide bounding box information for the hand.
                    </p>
                </li>
                <li>
                    <strong>Image Processing Parameters:</strong>
                    <p>
                        Several parameters are defined:
                        <ul>
                            <li><code>offset = 20</code>: This adds a margin around the hand's bounding box to ensure the entire hand is captured.</li>
                            <li><code>imgSize = 300</code>: This sets the desired size (in pixels) for the processed hand images, ensuring uniformity in the dataset.</li>
                            <li><code>counter = 0</code>: This variable tracks the number of images captured for each gesture.</li>
                            <li><code>folder = "C:\\Users\\VISHAL TAWASE\\Downloads\\VIT ALL SEM\\Winter Sem 2023-24\\ML for Robotics\\Project\\env\\env2\\Sign language detection\\Data\\Hello"</code>: This specifies the directory where the captured images will be saved. Make sure to replace this with the correct path corresponding to the gesture being captured. For example, if you are capturing the "Hello" gesture, you should point to the "Hello" folder inside the Data directory.</li>
                        </ul>
                    </p>
                </li>
                <li>
                    <strong>Main Loop and Hand Detection:</strong>
                    <p>
                        The <code>while True</code> loop continuously captures frames from the webcam. The <code>detector.findHands(img)</code> function detects hands in each frame, returning the hand data and the modified image.
                    </p>
                </li>
                <li>
                    <strong>Hand Bounding Box and Image Cropping:</strong>
                    <p>
                        If a hand is detected, the script extracts the bounding box coordinates (<code>x</code>, <code>y</code>, <code>w</code>, <code>h</code>) and crops the hand region from the original image with the added offset.
                    </p>
                </li>
                <li>
                    <strong>Image Resizing and Padding:</strong>
                    <p>
                        To ensure all hand images have the same size, the script resizes and pads the cropped image. It calculates the aspect ratio of the hand and resizes the image accordingly. Then, it creates a white canvas (<code>imgWhite</code>) of the desired size and places the resized hand image in the center, padding the remaining areas with white pixels. This ensures consistent input for the model.
                    </p>
                </li>
                <li>
                    <strong>Displaying Images:</strong>
                    <p>
                        The script displays the cropped hand image (<code>ImageCrop</code>) and the padded, resized image (<code>ImageWhite</code>) using <code>cv2.imshow()</code>. It also shows the original webcam feed (<code>Image</code>).
                    </p>
                </li>
                <li>
                    <strong>Saving Images:</strong>
                    <p>
                        When the user presses the 'S' key, the <code>imgWhite</code> image is saved to the specified folder with a unique filename (<code>Image_{time.time()}.jpg</code>). The <code>counter</code> is incremented, and the current count is printed to the console.
                    </p>
                </li>
                <li>
                    <strong>Loop Continuation:</strong>
                    <p>
                        The <code>cv2.waitKey(1)</code> function waits for a key press for 1 millisecond. If no key is pressed, the loop continues to the next frame.
                    </p>
                </li>
            </ol>
    
            <p>
                <strong>Important Note:</strong>
                Before running the script, ensure you have the necessary libraries installed (OpenCV and cvzone). Also, adjust the <code>folder</code> variable to match the correct directory for each gesture you want to capture.
            </p>
        </div>
    
        <p>
            ðŸ‘‰ Press <b>'S'</b> while running the <code>dataCollection.py</code> script to save an image of the current hand gesture. Capture multiple images for each gesture to create a robust dataset.
        </p>
    
        <h3>Project Folder Structure:</h3>
        <div class="code-box">
            <pre>
    Signlanguagedetection/
    â”‚â”€â”€ Data/
    â”‚   â”œâ”€â”€ Hello/
    â”‚   â”œâ”€â”€ No/
    â”‚   â”œâ”€â”€ Okay/
    â”‚   â”œâ”€â”€ Please/
    â”‚   â”œâ”€â”€ Thank you/
    â”‚   â”œâ”€â”€ Yes/
    â”‚â”€â”€ Model/
    â”‚   â”œâ”€â”€ keras_model.h5
    â”‚   â”œâ”€â”€ labels.txt
    â”‚â”€â”€ dataCollection.py
    â”‚â”€â”€ test.py
    â”‚â”€â”€ pyvenv.cfg
            </pre>
        </div>
    </div>
    
    <script>
        document.getElementById('detailsButton').addEventListener('click', function() {
            var detailsContent = document.getElementById('detailsContent');
            if (detailsContent.style.display === 'none') {
                detailsContent.style.display = 'block';
                this.textContent = 'Hide Detailed Explanation of dataCollection.py';
            } else {
                detailsContent.style.display = 'none';
                this.textContent = 'Show Detailed Explanation of dataCollection.py';
            }
        });
    </script>


    <div class="step">
        <span class="step-title">Step 2: Preprocessing</span>
        <p>The collected images are cropped and resized to a fixed size (300x300 pixels). This ensures uniform input for the model.</p>
        <p>Images are converted to grayscale or normalized for better training results.</p>
    </div>

    <div class="step">
        <span class="step-title">Step 3: Model Training</span>
        <p>We use a machine learning model (TensorFlow/Keras) to classify hand signs. The model learns patterns from the processed images.</p>
        <p>Train the model using Google's <b>Teachable Machine</b> for a user-friendly training experience.</p>
        <a href="https://teachablemachine.withgoogle.com/train/image" class="btn train-btn" target="_blank">Train Your Model</a>
    </div>

    <div class="step">
    <span class="step-title">Step 4: Real-Time Detection - Interpreting Hand Gestures with the Trained Model</span>
    <p>
        This step focuses on using the trained model to perform real-time sign language detection. We leverage OpenCV for capturing live video from the webcam, and the <code>cvzone</code> library for hand tracking and gesture classification. The trained Keras model (<code>keras_model.h5</code>) and its corresponding labels (<code>labels.txt</code>) are loaded to recognize the hand gestures in real time.
    </p>

    <button id="testDetailsButton">Show Detailed Explanation of test.py</button>

    <div id="testDetailsContent" style="display: none;">
        <h3>Detailed Explanation of the <code>test.py</code> Script:</h3>
        <p>
            The Python script <code>test.py</code> performs the following key operations:
        </p>
        <ol>
            <li>
                <strong>Initialization:</strong>
                <p>
                    The script initializes the webcam using <code>cv2.VideoCapture(0)</code>, creates a <code>HandDetector</code> object for hand tracking, and a <code>Classifier</code> object to load the trained model and labels.
                </p>
                <p>
                    <code>classifier = Classifier("C:\\Users\\VISHAL TAWASE\\Downloads\\VIT ALL SEM\\Winter Sem 2023-24\\ML for Robotics\\Project\\env\\env2\\Signlanguagedetection\\Model\\keras_model.h5", "C:\\Users\\VISHAL TAWASE\\Downloads\\VIT ALL SEM\\Winter Sem 2023-24\\ML for Robotics\\Project\\env\\env2\\Signlanguagedetection\\Model\\labels.txt")</code> loads the trained model.
                </p>
                <p>
                    <code>labels = ["Hello","Okay","Please","Thank you","Yes","No"]</code> defines the list of labels corresponding to the model's output.
                </p>
            </li>
            <li>
                <strong>Main Loop and Hand Detection:</strong>
                <p>
                    The <code>while True</code> loop continuously captures frames from the webcam. <code>detector.findHands(img)</code> detects hands in each frame.
                </p>
            </li>
            <li>
                <strong>Hand Processing:</strong>
                <p>
                    If a hand is detected, the script extracts the bounding box (<code>x</code>, <code>y</code>, <code>w</code>, <code>h</code>) and crops the hand region with an offset.
                </p>
            </li>
            <li>
                <strong>Image Resizing and Padding:</strong>
                <p>
                    Similar to the data collection script, the cropped hand image is resized and padded to a uniform size (<code>imgSize</code> = 300x300 pixels) on a white background. This ensures consistent input for the model.
                </p>
            </li>
            <li>
                <strong>Classification:</strong>
                <p>
                    <code>prediction, index = classifier.getPrediction(imgWhite, draw=False)</code> uses the trained model to predict the hand gesture. The <code>index</code> corresponds to the predicted label, and <code>prediction</code> contains the probabilities of each class.
                </p>
                <p>
                    <code>print(prediction, index)</code> displays the prediction results in the console.
                </p>
            </li>
            <li>
                <strong>Displaying Results:</strong>
                <p>
                    <code>cv2.rectangle(imgOutput,(x-offset,y-offset-70),(x -offset+400, y - offset+60-50),(0,255,0),cv2.FILLED)</code> displays a filled rectangle to hold the predicted label.
                </p>
                <p>
                    <code>cv2.putText(imgOutput,labels[index],(x,y-30),cv2.FONT_HERSHEY_COMPLEX,2,(0,0,0),2)</code> displays the predicted label text on the image.
                </p>
                <p>
                    <code>cv2.rectangle(imgOutput,(x-offset,y-offset),(x + w + offset, y+h + offset),(0,255,0),4)</code> draws a rectangle around the detected hand.
                </p>
                <p>
                    <code>cv2.imshow('ImageCrop', imgCrop)</code> and <code>cv2.imshow('ImageWhite', imgWhite)</code> display the cropped and processed hand images.
                </p>
                <p>
                    <code>cv2.imshow('Image', imgOutput)</code> displays the original webcam feed with the detected hand and predicted label.
                </p>
            </li>
            <li>
                <strong>Loop Continuation:</strong>
                <p>
                    <code>cv2.waitKey(1)</code> waits for a key press for 1 millisecond, allowing the loop to continue.
                </p>
            </li>
        </ol>

        <p>
            <strong>Important Note:</strong>
            Ensure that the paths to <code>keras_model.h5</code> and <code>labels.txt</code> are correct. The <code>cvzone</code> library and OpenCV must be installed.
        </p>
    </div>

    <p>
        The recognized sign is displayed along with its meaning and an image representation in real-time.
    </p>
</div>

<script>
    document.getElementById('testDetailsButton').addEventListener('click', function() {
        var testDetailsContent = document.getElementById('testDetailsContent');
        if (testDetailsContent.style.display === 'none') {
            testDetailsContent.style.display = 'block';
            this.textContent = 'Hide Detailed Explanation of test.py';
        } else {
            testDetailsContent.style.display = 'none';
            this.textContent = 'Show Detailed Explanation of test.py';
        }
    });
</script>

    <a href="{{ url_for('index') }}" class="btn start-btn">Start Demo</a>
</div>

<footer style="margin-top: 20px; padding: 10px; background: #007bff; color: white; text-align: center;">
    &copy; 2025 Sign Language Detection | Developed by Vishal Tawase
</footer>

</body>
</html>
